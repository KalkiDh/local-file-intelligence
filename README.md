# ğŸ§  Local File Intelligence

A powerful, offline **RAG (Retrieval-Augmented Generation)** system that lets you chat with your local documents. It uses a **Hybrid Architecture** to handle both small summaries and massive reports with high precision.

> **Privacy First:** 100% Local. No data leaves your machine. Powered by [Ollama](https://ollama.com/).

## ğŸš€ Key Features

* **Hybrid Retrieval Engine:**
    * **RAM Mode:** Small files are kept in memory for 100% accuracy and zero latency.
    * **Vector Mode:** Large files (>4000 chars) are automatically chunked and indexed in **ChromaDB** for scalable semantic search.
* **Multi-Format Intelligence:**
    * ğŸ“„ **PDFs:** Extracts text and parses structure.
    * ğŸ“Š **CSVs:** Auto-calculates row counts and formats data into readable Markdown tables.
    * ğŸ—„ï¸ **NoSQL/JSON:** Flattens nested JSON data into analyze-able records (great for logs & mongo dumps).
* **ğŸ§  Deep Summarizer (Map-Reduce):**
    * Can summarize massive documents (500+ pages) by breaking them down, summarizing chunks, and combining them into a master report.
* **Interactive CLI:**
    * **Focus Mode:** Lock the chat to a specific file to eliminate noise.
    * **Session Recorder:** Save your entire analysis to a `.txt` report.

## ğŸ—ï¸ Architecture

```mermaid
%%{init: {'theme': 'dark'}}%%
graph TD
    %% Styling
    classDef storage fill:#006064,stroke:#4dd0e1,stroke-width:2px,color:#ffffff;
    classDef logic fill:#bf360c,stroke:#ffab91,stroke-width:2px,color:#ffffff;
    classDef newFeat fill:#4a148c,stroke:#e1bee7,stroke-width:2px,color:#ffffff;

    subgraph Startup ["Phase 1: Ingestion"]
        Loader["File Loader"] -->|"PDF/Txt"| TextData["Raw Text"]
        Loader -->|"CSV"| CSVData["Metadata + Tables"]
        Loader -->|"JSON"| JSONData["Flattened JSONL"]:::newFeat
        
        TextData & CSVData & JSONData --> Router{"Size < 4000 chars?"}
        
        Router -->|Yes| RAM["RAM Storage"]:::storage
        Router -->|No| VectorDB[("ChromaDB")]:::storage
    end

    subgraph Loop ["Phase 2: Interaction"]
        User([User]) --> CmdCheck{"Command?"}
        CmdCheck -->|":deep"| MapReduce["Deep Map-Reduce Engine"]:::newFeat
        CmdCheck -->|":focus"| FocusedSearch["Targeted Search"]
        CmdCheck -->|"Query"| HybridSearch["Hybrid Context Builder"]:::logic
        
        MapReduce & FocusedSearch & HybridSearch --> Agent["Ollama Agent"]:::logic
        Agent --> User
    end
```

ğŸ› ï¸ Installation

### Prerequisites
- Python 3.10+
- Ollama installed and running (if using the included Ollama agent)
- Git

### 1. Clone the repository
# ğŸ§  Local File Intelligence

A powerful, offline **RAG (Retrieval-Augmented Generation)** system that lets you chat with your local documents. It uses a **Hybrid Architecture** to handle both small summaries and massive reports with high precision.

> **Privacy First:** 100% Local. No data leaves your machine. Powered by [Ollama](https://ollama.com/).

## ğŸš€ Key Features

- **Hybrid Retrieval Engine:**
  - **RAM Mode:** Small files are kept in memory for 100% accuracy and zero latency.
  - **Vector Mode:** Large files (>4000 chars) are automatically chunked and indexed in **ChromaDB** for scalable semantic search.
- **Multi-Format Intelligence:**
  - ğŸ“„ **PDFs:** Extracts text and parses structure.
  - ğŸ“Š **CSVs:** Auto-calculates row counts and formats data into readable Markdown tables.
  - ğŸ—„ï¸ **NoSQL/JSON:** Flattens nested JSON data into analyze-able records (great for logs & mongo dumps).
- **ğŸ§  Deep Summarizer (Map-Reduce):**
  - Can summarize massive documents (500+ pages) by breaking them down, summarizing chunks, and combining them into a master report.
- **Interactive CLI:**
  - **Focus Mode:** Lock the chat to a specific file to eliminate noise.
  - **Session Recorder:** Save your entire analysis to a `.txt` report.
### 4. Pull the model (if using Ollama)
Ensure Ollama is running, then pull the default model (for example Llama 3):
```bash
ollama pull llama3
```

## ğŸ–¥ï¸ Usage

1. Prepare data: place your documents (PDF, CSV, JSON, TXT) under the `data/` folder.
2. Run the CLI:
```bash
python main.py --files ./data
```

## ğŸ® CLI Commands
- `files` : List all loaded files and their storage status (RAM vs Vector).
- `:focus [filename]` : Lock the chat to a specific file (e.g., `:focus sales.csv`).
- `:deep [filename]` : Run a Deep Map-Reduce summary on a large document.
- `:save [name]` : Export the current chat history to a text file.
- `:all` : Return to global search mode (search all files).
- `exit` : Quit the application.

## ğŸ“‚ Project Structure

local_file_intelligence/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ agent.py            # LLM Interaction Handler
â”‚   â”œâ”€â”€ hybrid_manager.py   # Router for RAM vs Vector DB
â”‚   â””â”€â”€ summarizer.py       # Map-Reduce Engine for large docs
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ file_loader.py      # Parsers for PDF, CSV, JSON
â”‚   â”œâ”€â”€ chunker.py          # Semantic Text Splitter
â”‚   â””â”€â”€ vector_store.py     # ChromaDB wrapper
â”œâ”€â”€ data/                   # Your documents go here
â”œâ”€â”€ main.py                 # CLI Entry Point
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ LICENSE                 # MIT License
â””â”€â”€ README.md               # Documentation

## ğŸ¤ Contributing
Contributions are welcome!

1. Fork the project
2. Create your feature branch: `git checkout -b feature/AmazingFeature`
3. Commit your changes: `git commit -m 'Add some AmazingFeature'`
4. Push to the branch: `git push origin feature/AmazingFeature`
5. Open a Pull Request

## ğŸ“œ License
Distributed under the MIT License. See LICENSE for more information.
